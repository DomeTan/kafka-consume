debug=false
server.port=18888

## Kafka settings
#spring.kafka.bootstrap-servers=172.168.1.17:9092,172.168.1.16:9092,172.168.1.14:9092
#spring.kafka.bootstrap-servers=127.0.0.1:9092
spring.kafka.bootstrap-servers=172.168.1.12:9092
spring.kafka.template.default-topic=first_floor
kafka.template.task.default-topic=upload-task
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.group-id=first_floor

kafka.consumer.task.group-id=pull-task
spring.kafka.consumer.auto-offset-reset=latest
spring.kafka.listener.concurrency=7
# 自动提交
spring.kafka.consumer.enable-auto-commit=true
spring.kafka.consumer.auto-commit-interval=100
# 拉取数据大小
spring.kafka.consumer.fetch-min-size=40960
## 原文存储路径
first_floor.log.src.storage.path=/waner/original
## Log setting
logging.file.path=/var/log/kafkaconsume
logging.level.cn.waner.kafkaconsume=INFO
## HADOOP settings
first_floor.hdfs=hdfs://172.168.1.17:9000
first_floor.hdfs.path=hdfs://172.168.1.17:9000/first_floor
## Redis setting
spring.redis.host=127.0.0.1
spring.redis.port=6379
spring.redis.database=0

## Elasticsearch settings
# Java High Level REST Client
#spring.elasticsearch.rest.uris=["http://172.168.1.214:9200","http://172.168.1.213:9200","http://172.168.1.212:9200","http://172.168.1.211:9200","http://172.168.1.211:9200"]
#spring.elasticsearch.rest.uris=["http://172.168.1.178:9200"]
#spring.data.elasticsearch.repositories.enabled=true
#spring.elasticsearch.rest.username=elastic
#spring.elasticsearch.rest.password=Waner123456
spring.data.elasticsearch.cluster-nodes=172.168.1.178:9200
# ===================================================


